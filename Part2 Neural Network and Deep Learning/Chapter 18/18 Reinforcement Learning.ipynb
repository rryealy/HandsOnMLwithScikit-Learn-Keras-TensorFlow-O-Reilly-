{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning\n",
        "## Chapter 18 - From Theory to Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Reinforcement Learning\n",
        "\n",
        "**Key Components**:\n",
        "- Agent: Learns to make decisions\n",
        "- Environment: Where the agent operates\n",
        "- State (s): Current situation\n",
        "- Action (a): Decision taken by agent\n",
        "- Reward (r): Feedback from environment\n",
        "- Policy (π): Strategy for acting in states\n",
        "\n",
        "**The RL Loop**:\n",
        "1. Agent observes state\n",
        "2. Selects action based on policy\n",
        "3. Receives reward and new state\n",
        "4. Updates policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core RL Algorithms\n",
        "\n",
        "### 2.1 Q-Learning\n",
        "Off-policy TD learning that estimates the optimal Q-values:\n",
        "\\[\n",
        "Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]\n",
        "\\]\n",
        "\n",
        "- $\\alpha$: Learning rate\n",
        "- $\\gamma$: Discount factor\n",
        "- $\\epsilon$: Exploration rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Q-table initialization\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "n_episodes = 1000\n",
        "\n",
        "# Q-learning implementation\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q[state])  # Exploit\n",
        "            \n",
        "        # Take action\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "        \n",
        "        # Q-table update\n",
        "        Q[state, action] = Q[state, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[new_state]) - Q[state, action]\n",
        "        )\n",
        "        \n",
        "        state = new_state\n",
        "\n",
        "print(\"Q-table after training:\")\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Deep Q-Networks (DQN)\n",
        "Uses neural networks to approximate Q-values. Key innovations:\n",
        "- Experience replay\n",
        "- Target network\n",
        "- Frame stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0   # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        \n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(24, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "    \n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = self.model.predict(state)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                t = self.target_model.predict(next_state)\n",
        "                target[0][action] = reward + self.gamma * np.amax(t[0])\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Policy Gradient Methods\n",
        "\n",
        "Directly optimize the policy $\\pi_θ(a|s)$. The REINFORCE algorithm:\n",
        "\\[\n",
        "\\nabla_θ J(θ) = \\mathbb{E}[\\nabla_θ \\log \\pi_θ(a|s) G_t]\n",
        "\\]\n",
        "\n",
        "Where $G_t$ is the return from time step $t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PGAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = 0.95\n",
        "        self.learning_rate = 0.001\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.model = self._build_model()\n",
        "        \n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(24, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='softmax')\n",
        "        ])\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "    \n",
        "    def remember(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = np.reshape(state, [1, self.state_size])\n",
        "        prob_weights = self.model.predict(state)\n",
        "        action = np.random.choice(self.action_size, p=prob_weights[0])\n",
        "        return action\n",
        "    \n",
        "    def train(self):\n",
        "        discounted_rewards = []\n",
        "        Gt = 0\n",
        "        for reward in reversed(self.rewards):\n",
        "            Gt = reward + self.gamma * Gt\n",
        "            discounted_rewards.insert(0, Gt)\n",
        "        \n",
        "        # Normalize rewards\n",
        "        discounted_rewards = np.array(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / np.std(discounted_rewards)\n",
        "        \n",
        "        # Convert actions to one-hot vectors\n",
        "        actions = tf.keras.utils.to_categorical(self.actions, num_classes=self.action_size)\n",
        "        \n",
        "        # Update policy\n",
        "        self.model.train_on_batch(np.vstack(self.states), actions, sample_weight=discounted_rewards)\n",
        "        \n",
        "        # Clear memory\n",
        "        self.states, self.actions, self.rewards = [], [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Proximal Policy Optimization (PPO)\n",
        "\n",
        "Modern policy gradient algorithm with:\n",
        "- Clipped surrogate objective\n",
        "- Multiple epochs per rollout\n",
        "- Advantage normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def store(self, state, action, prob, val, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(prob)\n",
        "        self.vals.append(val)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, action_std_init=0.6):\n",
        "        self.policy = self._build_network(state_dim, action_dim)\n",
        "        self.policy_old = self._build_network(state_dim, action_dim)\n",
        "        self.memory = PPOMemory()\n",
        "        \n",
        "    def _build_network(self, state_dim, action_dim):\n",
        "        # Simplified network architecture\n",
        "        inputs = tf.keras.layers.Input(shape=(state_dim,))\n",
        "        x = tf.keras.layers.Dense(64, activation='tanh')(inputs)\n",
        "        x = tf.keras.layers.Dense(64, activation='tanh')(x)\n",
        "        mean = tf.keras.layers.Dense(action_dim, activation='tanh')(x)\n",
        "        std = tf.keras.layers.Dense(action_dim, activation='softplus')(x)\n",
        "        return tf.keras.Model(inputs, [mean, std])\n",
        "    \n",
        "    def update(self):\n",
        "        # PPO update logic would go here\n",
        "        # Includes: advantages calculation, clipping, multiple epochs\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Practical Considerations\n",
        "\n",
        "### 5.1 Reward Shaping\n",
        "- Design rewards to guide learning\n",
        "- Balance sparse vs dense rewards\n",
        "- Avoid reward hacking\n",
        "\n",
        "### 5.2 Exploration Strategies\n",
        "- ε-greedy\n",
        "- Boltzmann exploration\n",
        "- Noisy networks\n",
        "- Intrinsic curiosity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exercises\n",
        "\n",
        "1. Implement Double DQN to reduce overestimation bias\n",
        "2. Add prioritized experience replay to the DQN\n",
        "3. Train a policy gradient agent on CartPole-v1\n",
        "4. Compare performance of different exploration strategies\n",
        "5. Visualize learned value functions and policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Takeaways (Continued)\n",
        "\n",
        "- **Value-based methods** (Q-learning, DQN) learn optimal value functions\n",
        "- **Policy-based methods** (REINFORCE, PPO) directly optimize policies\n",
        "- **Model-based RL** learns environment dynamics for planning\n",
        "- **Exploration vs Exploitation** must be carefully balanced\n",
        "- **Reward design** critically impacts learning success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Implementing RL with TF-Agents\n",
        "\n",
        "TensorFlow Agents provides production-grade RL implementations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "# Create environment\n",
        "env = suite_gym.load('CartPole-v1')\n",
        "train_env = suite_gym.load('CartPole-v1')\n",
        "\n",
        "# Create DQN Agent\n",
        "q_net = q_network.QNetwork(\n",
        "    env.observation_spec(),\n",
        "    env.action_spec(),\n",
        "    fc_layer_params=(100,))\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    env.time_step_spec(),\n",
        "    env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
        "\n",
        "# Create replay buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=100000)\n",
        "\n",
        "print(\"TF-Agents DQN initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop Example\n",
        "\n",
        "Complete training procedure with TF-Agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(agent, env, replay_buffer, num_iterations=20000):\n",
        "    # Create dataset from replay buffer\n",
        "    dataset = replay_buffer.as_dataset(\n",
        "        num_parallel_calls=3,\n",
        "        sample_batch_size=64,\n",
        "        num_steps=2).prefetch(3)\n",
        "    \n",
        "    # Training loop\n",
        "    for _ in range(num_iterations):\n",
        "        # Collect experience\n",
        "        time_step = env.current_time_step()\n",
        "        action_step = agent.policy.action(time_step)\n",
        "        next_time_step = env.step(action_step.action)\n",
        "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "        replay_buffer.add_batch(traj)\n",
        "        \n",
        "        # Train on sampled experience\n",
        "        experience, _ = next(iter(dataset))\n",
        "        train_loss = agent.train(experience).loss\n",
        "        \n",
        "        if _ % 1000 == 0:\n",
        "            print(f'Iteration {_}: Loss = {train_loss}')\n",
        "    \n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Advanced RL Techniques\n",
        "\n",
        "### 9.1 Imitation Learning\n",
        "- Learn from expert demonstrations\n",
        "- Behavioral cloning\n",
        "- Dataset Aggregation (DAgger)\n",
        "\n",
        "### 9.2 Multi-Agent RL\n",
        "- Competitive/cooperative environments\n",
        "- Markov Games framework\n",
        "- Centralized training with decentralized execution\n",
        "\n",
        "### 9.3 Hierarchical RL\n",
        "- Temporal abstraction with options\n",
        "- Meta-controllers and sub-policies\n",
        "- Feudal networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Debugging RL Systems\n",
        "\n",
        "Common issues and solutions:\n",
        "\n",
        "| Problem | Possible Causes | Solutions |\n",
        "|---------|-----------------|-----------|\n",
        "| No learning | Low exploration, bad rewards | Adjust ε, reshape rewards |\n",
        "| Unstable training | High learning rate, small buffer | Reduce LR, increase buffer |\n",
        "| Poor final performance | Limited capacity, local optima | Larger network, better exploration |\n",
        "| High variance | Small batches, no target network | Increase batch size, add target net |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Summary\n",
        "\n",
        "- **Tabular methods** work well for small state spaces\n",
        "- **Deep RL** scales to complex environments\n",
        "- **Policy gradients** handle continuous actions naturally\n",
        "- **TF-Agents** provides production-ready implementations\n",
        "- **Careful experimentation** is key to successful RL applications"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
