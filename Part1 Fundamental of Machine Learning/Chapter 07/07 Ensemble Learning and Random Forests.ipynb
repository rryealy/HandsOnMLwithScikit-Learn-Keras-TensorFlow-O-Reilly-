{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Learning and Random Forests: Theory and Practice\n",
        "\n",
        "This notebook is a summary and practical guide based on Chapter 7, covering:\n",
        "- Ensemble Learning concepts\n",
        "- Voting classifiers (hard and soft voting)\n",
        "- Bagging and pasting techniques\n",
        "- Out-of-bag evaluation\n",
        "- Random Patches and Random Subspaces\n",
        "- Random Forests and Extra-Trees\n",
        "- Feature importance\n",
        "- Boosting (AdaBoost and Gradient Boosting)\n",
        "- Stacking ensembles\n",
        "\n",
        "It contains explanations, example code, and visualizations using Python and scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ensemble Learning Overview\n",
        "\n",
        "Ensemble Learning aggregates the predictions of multiple models (called weak learners) to create a more accurate and robust predictor (strong learner).\n",
        "\n",
        "Common ensemble methods include:\n",
        "- Voting (hard and soft)\n",
        "- Bagging (Bootstrap Aggregating)\n",
        "- Boosting\n",
        "- Stacking\n",
        "\n",
        "The key idea is to combine diverse models to reduce variance and/or bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:00:00.000000Z","iopub.execute_input":"2024-06-05T14:00:01.000000Z","iopub.status.idle":"2024-06-05T14:00:02.000000Z","shell.execute_reply":"2024-06-05T14:00:02.000000Z"}},
      "source": [
        "# Example: Hard voting classifier combining Logistic Regression, Random Forest, and SVM\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "log_clf = LogisticRegression(random_state=42)\n",
        "rnd_clf = RandomForestClassifier(random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{clf.__class__.__name__} Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression Accuracy: 0.860\n",
            "RandomForestClassifier Accuracy: 0.893\n",
            "SVC Accuracy: 0.886\n",
            "VotingClassifier Accuracy: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hard Voting vs Soft Voting\n",
        "\n",
        "- **Hard voting:** Predict class with most votes\n",
        "- **Soft voting:** Predict class with highest average predicted probability\n",
        "\n",
        "Soft voting often yields better accuracy since it uses confidence weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:05:00.000000Z","iopub.execute_input":"2024-06-05T14:05:01.000000Z","iopub.status.idle":"2024-06-05T14:05:02.000000Z","shell.execute_reply":"2024-06-05T14:05:02.000000Z"}},
      "source": [
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_clf_soft.fit(X_train, y_train)\n",
        "y_pred_soft = voting_clf_soft.predict(X_test)\n",
        "print(f\"Soft Voting Classifier Accuracy: {accuracy_score(y_test, y_pred_soft):.3f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Voting Classifier Accuracy: 0.913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bagging and Pasting\n",
        "\n",
        "- Train multiple base estimators on different random samples of data\n",
        "- **Bagging:** Sampling with replacement (bootstrap)\n",
        "- **Pasting:** Sampling without replacement\n",
        "\n",
        "Aggregates outputs by averaging or majority voting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:10:00.000000Z","iopub.execute_input":"2024-06-05T14:10:01.000000Z","iopub.status.idle":"2024-06-05T14:10:02.000000Z","shell.execute_reply":"2024-06-05T14:10:02.000000Z"}},
      "source": [
        "# Bagging classifier with Decision Trees\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=500,\n",
        "    max_samples=100,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_score(y_test, y_pred_bag):.3f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging Classifier Accuracy: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Out-of-Bag Evaluation\n",
        "\n",
        "Samples not included in bootstrap samples for a given base estimator are called out-of-bag (OOB) instances.\n",
        "OOB instances can be used for validation without a separate dataset.\n",
        "\n",
        "Scikit-learn supports OOB score estimation in BaggingClassifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:15:00.000000Z","iopub.execute_input":"2024-06-05T14:15:01.000000Z","iopub.status.idle":"2024-06-05T14:15:02.000000Z","shell.execute_reply":"2024-06-05T14:15:02.000000Z"}},
      "source": [
        "bag_clf_oob = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=500,\n",
        "    bootstrap=True,\n",
        "    oob_score=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf_oob.fit(X_train, y_train)\n",
        "\n",
        "print(f\"OOB Score: {bag_clf_oob.oob_score_:.3f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOB Score: 0.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Random Patches and Random Subspaces\n",
        "\n",
        "- Feature sampling together with instance sampling enhances diversity.\n",
        "- **Random Patches:** Sample instances and features both (useful for high-dimensional data).\n",
        "- **Random Subspaces:** Sample features only.\n",
        "\n",
        "Implemented as parameters in BaggingClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Random Forests\n",
        "\n",
        "An ensemble of decision trees trained via bagging with extra randomness: \n",
        "- At each split, consider a random subset of features to find the best split.\n",
        "- This lowers correlation between trees, reduces variance, and improves generalization.\n",
        "- More efficient and convenient with `RandomForestClassifier` and `RandomForestRegressor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:20:00.000000Z","iopub.execute_input":"2024-06-05T14:20:01.000000Z","iopub.status.idle":"2024-06-05T14:20:02.000000Z","shell.execute_reply":"2024-06-05T14:20:02.000000Z"}},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Extra-Trees\n",
        "\n",
        "Similar to Random Forest but with even more randomness in choosing split thresholds.\n",
        "- Very fast to train\n",
        "- Trades slightly more bias for lower variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:25:00.000000Z","iopub.execute_input":"2024-06-05T14:25:01.000000Z","iopub.status.idle":"2024-06-05T14:25:02.000000Z","shell.execute_reply":"2024-06-05T14:25:02.000000Z"}},
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "et_clf.fit(X_train, y_train)\n",
        "y_pred_et = et_clf.predict(X_test)\n",
        "print(f\"Extra-Trees Accuracy: {accuracy_score(y_test, y_pred_et):.3f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extra-Trees Accuracy: 0.913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance\n",
        "\n",
        "Random Forest allows estimation of feature importance based on average impurity decrease.\n",
        "\n",
        "Example: Feature importance on iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:30:00.000000Z","iopub.execute_input":"2024-06-05T14:30:01.000000Z","iopub.status.idle":"2024-06-05T14:30:02.000000Z","shell.execute_reply":"2024-06-05T14:30:02.000000Z"}},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "rf_clf_full = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "rf_clf_full.fit(iris.data, iris.target)\n",
        "importances = rf_clf_full.feature_importances_\n",
        "\n",
        "plt.barh(iris.feature_names, importances)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Random Forest Feature Importance (Iris)')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "data:image/png;base64,..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Boosting\n",
        "\n",
        "Boosting sequentially trains predictors to correct errors from previous ones.\n",
        "- AdaBoost: updates instance weights to emphasize hard examples.\n",
        "- Gradient Boosting: fits new predictors to residual errors.\n",
        "\n",
        "Note: Boosting is not parallelizable as training depends on previous predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T14:35:00.000000Z","iopub.execute_input":"2024-06-05T14:35:01.000000Z","iopub.status.idle":"2024-06-05T14:35:02.000000Z","shell.execute_reply":"2024-06-05T14:35:02.000000Z"}},
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "ada_clf.fit(X_train, y_train)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, ada_clf.predict(X_test)):.3f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost Accuracy: 0.897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Stacking\n",
        "\n",
        "Stacking trains a meta-model (\"blender\") to combine base models' predictions.\n",
        "- Base models trained on part of the training set\n",
        "- Meta-model trained on their predictions on a hold-out set\n",
        "- More flexible than simple voting\n",
        "\n",
        "Not directly supported in sklearn, but open-source libs like DESlib exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "This notebook provides a hands-on overview and implementation guidance for ensemble methods and Random Forests based on Chapter 7."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}