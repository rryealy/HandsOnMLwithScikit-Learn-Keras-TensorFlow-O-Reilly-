{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artificial Neural Networks: Comprehensive Summary\n",
        "## Chapter 10 - Neural Network Fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Fundamental Concepts\n",
        "- **Biological Inspiration**: ANNs are inspired by the structure and function of biological neurons in the human brain. Each neuron receives inputs, processes them, and produces an output, similar to how biological neurons communicate through synapses.\n",
        "- **Basic Components**:\n",
        "  - **Input Layer**: The first layer that receives the input features.\n",
        "  - **Hidden Layers**: Intermediate layers that transform inputs into outputs through weighted connections and activation functions.\n",
        "  - **Output Layer**: The final layer that produces the output predictions.\n",
        "  - **Weights and Biases**: Parameters that are adjusted during training to minimize the error in predictions.\n",
        "  - **Activation Functions**: Functions applied to the output of each neuron to introduce non-linearity, allowing the network to learn complex patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core Components\n",
        "### 2.1 Neuron Structure\n",
        "- **Mathematical Model**: Each neuron computes a weighted sum of its inputs and applies an activation function:\n",
        "  \\[ output = activation(\\sum (weights \\times inputs) + bias) \\]\n",
        "- **Common Activation Functions**:\n",
        "  - **ReLU (Rectified Linear Unit)**: Outputs the input directly if positive; otherwise, it outputs zero. It helps mitigate the vanishing gradient problem.\n",
        "  - **Sigmoid**: Maps input to a value between 0 and 1, useful for binary classification.\n",
        "  - **Tanh**: Similar to sigmoid but outputs values between -1 and 1, centering the data.\n",
        "  - **Softmax**: Converts logits to probabilities for multi-class classification, ensuring the outputs sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Activation Function Visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "def relu(x): return np.maximum(0, x)\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "def tanh(x): return np.tanh(x)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(x, relu(x))\n",
        "plt.title('ReLU Activation')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(x, sigmoid(x))\n",
        "plt.title('Sigmoid Activation')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(x, tanh(x))\n",
        "plt.title('Tanh Activation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Process\n",
        "### 3.1 Backpropagation Algorithm\n",
        "- **Forward Pass**: The input data is passed through the network, layer by layer, to compute the output predictions.\n",
        "- **Loss Calculation**: The difference between the predicted output and the actual target is calculated using a loss function (e.g., Mean Squared Error for regression, Cross-Entropy for classification).\n",
        "- **Backward Pass**: The algorithm computes the gradient of the loss with respect to each weight by applying the chain rule, allowing the model to update weights to minimize the loss.\n",
        "- **Gradient Descent**: An optimization algorithm used to adjust the weights based on the computed gradients, iteratively improving the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Neural Network with TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and prepare data\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Architectural Variations\n",
        "### 4.1 Multilayer Perceptrons (MLP)\n",
        "- **Definition**: A type of ANN with one or more hidden layers, where each layer is fully connected to the next.\n",
        "- **Functionality**: MLPs can approximate any continuous function given enough neurons and layers.\n",
        "\n",
        "### 4.2 Deep Neural Networks\n",
        "- **Definition**: Networks with multiple hidden layers (more than two).\n",
        "- **Benefits**: Capable of learning complex representations and features from data, especially with large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Practical Considerations\n",
        "- **Hyperparameter Tuning**:\n",
        "  - **Number of Layers and Units**: More layers can capture more complex patterns, but risk overfitting.\n",
        "  - **Learning Rate**: A critical parameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
        "  - **Batch Size**: The number of training examples utilized in one iteration. Smaller batch sizes can lead to better generalization.\n",
        "  - **Regularization Techniques**: Such as Dropout (randomly dropping units during training) and L2 regularization (penalizing large weights).\n",
        "- **Common Challenges**:\n",
        "  - **Vanishing/Exploding Gradients**: Issues that can occur during training of deep networks, where gradients become too small or too large.\n",
        "  - **Overfitting**: When the model learns noise in the training data instead of the actual pattern, leading to poor performance on unseen data.\n",
        "  - **Computational Requirements**: Training deep networks can be resource-intensive, requiring powerful hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Real-World Applications\n",
        "- **Image Classification**: Identifying objects within images (e.g., facial recognition).\n",
        "- **Natural Language Processing**: Understanding and generating human language (e.g., chatbots).\n",
        "- **Time Series Forecasting**: Predicting future values based on past data (e.g., stock prices).\n",
        "- **Recommendation Systems**: Suggesting products or content based on user behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Takeaways\n",
        "- **Powerful Function Approximators**: ANNs can model complex relationships in data.\n",
        "- **Need for Careful Tuning**: Hyperparameters significantly affect performance.\n",
        "- **Frameworks**: Modern libraries like TensorFlow and PyTorch simplify the implementation of neural networks.\n",
        "- **Effectiveness**: Particularly strong in tasks involving complex pattern recognition."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
