{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Natural Language Processing with RNNs and Attention\n",
        "## Chapter 16 - NLP Implementation Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to NLP\n",
        "\n",
        "Natural Language Processing (NLP) enables machines to understand, interpret, and generate human language. Key applications include:\n",
        "- Machine translation\n",
        "- Sentiment analysis\n",
        "- Text generation\n",
        "- Question answering\n",
        "\n",
        "**Core Challenges**:\n",
        "- Variable-length sequences\n",
        "- Contextual meaning\n",
        "- Ambiguity and polysemy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Preprocessing\n",
        "\n",
        "### 2.1 Tokenization\n",
        "Convert text to numerical representations:\n",
        "- Word-level\n",
        "- Character-level\n",
        "- Subword (BPE, WordPiece)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample text data\n",
        "texts = [\"Natural language processing is fascinating\",\n",
        "         \"Deep learning models can understand text\"]\n",
        "\n",
        "# Create and fit tokenizer\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "\n",
        "print(\"Word index:\", tokenizer.word_index)\n",
        "print(\"Padded sequences:\\n\", padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Embedding Layers\n",
        "Map tokens to dense vectors capturing semantic meaning:\n",
        "- Pretrained embeddings (GloVe, Word2Vec)\n",
        "- Trainable embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=100,  # Vocabulary size\n",
        "    output_dim=64,  # Embedding dimension\n",
        "    input_length=10  # Max sequence length\n",
        ")\n",
        "\n",
        "# Example usage\n",
        "import numpy as np\n",
        "sample_input = np.random.randint(0, 100, size=(32, 10))\n",
        "embedded = embedding_layer(sample_input)\n",
        "print(\"Embedded shape:\", embedded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sequence Models for NLP\n",
        "\n",
        "### 3.1 RNN-based Models\n",
        "- Process text sequentially\n",
        "- Capture temporal dependencies\n",
        "- Variants: LSTM, GRU (address vanishing gradient problem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# LSTM model for text classification\n",
        "model = Sequential([\n",
        "    Embedding(10000, 128, input_length=100),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 CNN for Text\n",
        "- Process text as 1D signals\n",
        "- Can capture local patterns effectively\n",
        "- Often combined with RNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# TextCNN model\n",
        "textcnn = Sequential([\n",
        "    Embedding(10000, 128, input_length=100),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "textcnn.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "textcnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Attention Mechanisms\n",
        "\n",
        "### 4.1 Basic Attention\n",
        "- Allows models to focus on relevant parts of input\n",
        "- Computes context vectors as weighted sums\n",
        "- Particularly useful for long sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, query, values):\n",
        "        # Hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # Score shape == (batch_size, max_length, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Example usage\n",
        "attention_layer = BahdanauAttention(10)\n",
        "query = tf.random.normal((1, 20))  # Decoder hidden state\n",
        "values = tf.random.normal((1, 50, 20))  # Encoder outputs\n",
        "context_vector, attention_weights = attention_layer(query, values)\n",
        "print(\"Context vector shape:\", context_vector.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transformer Architecture\n",
        "\n",
        "### 5.1 Key Components\n",
        "- Self-attention mechanism\n",
        "- Multi-head attention\n",
        "- Positional encoding\n",
        "- Layer normalization\n",
        "\n",
        "### 5.2 Implementation Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Example usage\n",
        "transformer_block = TransformerBlock(embed_dim=32, num_heads=2, ff_dim=64)\n",
        "print(transformer_block(tf.random.uniform((1, 10, 32)), training=False).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Practical Applications\n",
        "\n",
        "### 6.1 Sentiment Analysis\n",
        "- Classify text sentiment (positive/negative)\n",
        "- Can use RNNs, CNNs, or Transformers\n",
        "\n",
        "### 6.2 Neural Machine Translation\n",
        "- Encoder-decoder architecture\n",
        "- Attention improves performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exercises\n",
        "\n",
        "1. Implement a character-level RNN for text generation\n",
        "2. Compare word vs. subword tokenization\n",
        "3. Add attention to a sequence-to-sequence model\n",
        "4. Fine-tune a pretrained transformer model\n",
        "5. Visualize attention weights for sample inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "- NLP requires specialized preprocessing and tokenization\n",
        "- RNNs and CNNs can effectively process text sequences\n",
        "- Attention mechanisms enable models to focus on relevant context\n",
        "- Transformers have become the state-of-the-art for many tasks\n",
        "- Pretrained models (BERT, GPT) provide powerful starting points"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
