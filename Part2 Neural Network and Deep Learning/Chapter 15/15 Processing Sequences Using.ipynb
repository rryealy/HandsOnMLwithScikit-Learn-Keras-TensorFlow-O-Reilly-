{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Processing Sequences with RNNs and CNNs\n",
        "## Chapter 15 - Sequence Modeling Implementation Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Sequence Processing\n",
        "\n",
        "Sequence models are crucial for:\n",
        "- Time series forecasting\n",
        "- Natural language processing\n",
        "- Speech recognition\n",
        "- Music generation\n",
        "\n",
        "Key challenges:\n",
        "- Variable-length inputs/outputs\n",
        "- Long-term dependencies\n",
        "- Temporal patterns at different scales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Recurrent Neural Networks (RNNs)\n",
        "\n",
        "### 2.1 Basic RNN Architecture\n",
        "- Process sequences step-by-step\n",
        "- Maintain hidden state between time steps\n",
        "- Mathematical formulation:\n",
        "  \\[\n",
        "  h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\n",
        "  \\]\n",
        "  \\[\n",
        "  y_t = W_{hy}h_t + b_y\n",
        "  \\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "# Basic RNN example\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(units=32, input_shape=(None, 10)),  # Any sequence length\n",
        "    tf.keras.layers.Dense(1)  # Single output prediction\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Long Short-Term Memory (LSTM)\n",
        "- Solves vanishing gradient problem\n",
        "- Uses gating mechanisms:\n",
        "  - Forget gate (what to discard)\n",
        "  - Input gate (what to store)\n",
        "  - Output gate (what to output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM implementation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(None, 10)),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sequence-to-Sequence Models\n",
        "\n",
        "### 3.1 Encoder-Decoder Architecture\n",
        "- Encoder processes input sequence\n",
        "- Decoder generates output sequence\n",
        "- Used for machine translation, summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Encoder-Decoder implementation\n",
        "encoder_inputs = tf.keras.Input(shape=(None, 10))\n",
        "encoder = tf.keras.layers.LSTM(32, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.Input(shape=(None, 8))  # Different output features\n",
        "decoder_lstm = tf.keras.layers.LSTM(32, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(8, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Temporal Convolutional Networks\n",
        "\n",
        "### 4.1 1D Convolution for Sequences\n",
        "- Process sequences with CNNs\n",
        "- Benefits:\n",
        "  - Parallel processing\n",
        "  - Long-term pattern capture with dilated convolutions\n",
        "  - Stable gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WaveNet-like architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "\n",
        "# Add dilated convolutional blocks\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(tf.keras.layers.Conv1D(\n",
        "        filters=20, kernel_size=2, padding=\"causal\",\n",
        "        activation=\"relu\", dilation_rate=rate))\n",
        "\n",
        "# Add final layers\n",
        "model.add(tf.keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Attention Mechanisms\n",
        "\n",
        "### 5.1 Attention Layer\n",
        "- Focus on relevant parts of input\n",
        "- Computes context vector as weighted sum\n",
        "- Transformers use self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified attention implementation\n",
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, query, values):\n",
        "        # Score calculation\n",
        "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))\n",
        "        \n",
        "        # Attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # Context vector\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Example usage\n",
        "attention = AttentionLayer(10)\n",
        "query = tf.random.normal((32, 1, 64))\n",
        "values = tf.random.normal((32, 10, 64))\n",
        "context, weights = attention(query, values)\n",
        "print(\"Context vector shape:\", context.shape)\n",
        "print(\"Attention weights shape:\", weights.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Practical Applications\n",
        "\n",
        "### 6.1 Time Series Forecasting\n",
        "- Predict future values from historical data\n",
        "- Example: Stock prices, weather, sales\n",
        "\n",
        "### 6.2 Text Generation\n",
        "- Character-level or word-level models\n",
        "- Can capture style and structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text generation example\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Create character-level dataset\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create training examples\n",
        "seq_length = 10\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training batches\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = dataset.shuffle(10000).batch(1, drop_remainder=True)\n",
        "\n",
        "# Build and train model (simplified example)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(vocab), 8, batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(32, return_sequences=True, stateful=True),\n",
        "    tf.keras.layers.Dense(len(vocab))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exercises\n",
        "\n",
        "1. Implement an LSTM for time series forecasting\n",
        "2. Compare performance of RNN vs CNN for sequence processing\n",
        "3. Add attention to a sequence-to-sequence model\n",
        "4. Experiment with different RNN cell types (GRU, LSTM, SimpleRNN)\n",
        "5. Implement a character-level text generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "- RNNs process sequences step-by-step but can struggle with long-term dependencies\n",
        "- LSTMs and GRUs address vanishing gradient problems with gating mechanisms\n",
        "- 1D CNNs can effectively process sequences with advantages over RNNs\n",
        "- Attention mechanisms help focus on relevant parts of input sequences\n",
        "- Sequence models power applications from forecasting to text generation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
