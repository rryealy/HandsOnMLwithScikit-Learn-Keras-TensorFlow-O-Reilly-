{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Models and Training with TensorFlow\n",
        "## Chapter 12 - Advanced TensorFlow Implementation Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to TensorFlow Customization\n",
        "\n",
        "TensorFlow provides multiple levels of abstraction for building and training models. This chapter focuses on going beyond the high-level Keras API to create custom components and training loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Custom Loss Functions\n",
        "\n",
        "### 2.1 Creating Custom Losses\n",
        "TensorFlow allows you to define your own loss functions when standard ones don't meet your needs.\n",
        "\n",
        "**Example: Huber Loss Implementation**\n",
        "\n",
        "The Huber loss is less sensitive to outliers than MSE:\n",
        "\\[\n",
        "L_{\\delta}(y, \\hat{y}) = \\begin{cases} \n",
        "\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
        "\\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < delta\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = delta * (tf.abs(error) - delta/2)\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# Test the function\n",
        "y_true = tf.constant([0., 1., 2.])\n",
        "y_pred = tf.constant([0.1, 1.2, 1.9])\n",
        "print(\"Huber loss:\", huber_loss(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Using Custom Losses in Keras Models\n",
        "\n",
        "You can use custom loss functions just like built-in ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(10, activation='relu', input_shape=(8,)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=huber_loss)\n",
        "print(\"Model compiled with custom Huber loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Layers\n",
        "\n",
        "### 3.1 Implementing a Custom Layer\n",
        "\n",
        "Create layers by subclassing `tf.keras.layers.Layer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_normal',\n",
        "            trainable=True)\n",
        "        self.bias = self.add_weight(\n",
        "            name='bias',\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=True)\n",
        "        super().build(input_shape)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.kernel) + self.bias)\n",
        "    \n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, 'units': self.units, 'activation': tf.keras.activations.serialize(self.activation)}\n",
        "\n",
        "# Test the custom layer\n",
        "custom_layer = MyDenseLayer(32, activation='relu')\n",
        "print(\"Output shape:\", custom_layer(tf.zeros([1, 10])).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Custom Training Loops\n",
        "\n",
        "### 4.1 Basic Training Loop Structure\n",
        "\n",
        "When you need more control than `model.fit()`, implement your own loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(8,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "x_train = np.random.random((1000, 8))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nStart of epoch {epoch + 1}\")\n",
        "    \n",
        "    for step, (x_batch, y_batch) in enumerate(dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss_value = loss_fn(y_batch, logits)\n",
        "        \n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        \n",
        "        if step % 10 == 0:\n",
        "            print(f\"Training loss at step {step}: {float(loss_value):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Using tf.function for Performance\n",
        "\n",
        "### 5.1 Converting Python Functions to TensorFlow Graphs\n",
        "\n",
        "`@tf.function` decorator converts Python functions to optimized TensorFlow graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def my_function(x):\n",
        "    return tf.reduce_sum(x ** 2)\n",
        "\n",
        "x = tf.constant([1.0, 2.0, 3.0])\n",
        "print(\"Function output:\", my_function(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exercises\n",
        "\n",
        "1. Implement a custom layer that performs layer normalization\n",
        "2. Create a custom metric that tracks precision for a binary classifier\n",
        "3. Modify the training loop to include learning rate scheduling\n",
        "4. Benchmark a model with and without `@tf.function` to see the performance difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Takeaways\n",
        "\n",
        "- TensorFlow provides flexible APIs for customizing every aspect of model building and training\n",
        "- Custom components should subclass appropriate Keras base classes for compatibility\n",
        "- Custom training loops offer maximum control but require more code\n",
        "- `@tf.function` can significantly improve performance by converting Python to graph execution"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
