{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning: Training Models\n",
        "\n",
        "This notebook summarizes key theoretical concepts and practical examples from Chapter 4 on training models, covering:\n",
        "- Linear Regression and its training algorithms\n",
        "- Gradient Descent variants\n",
        "- Polynomial Regression\n",
        "- Regularization: Ridge, Lasso, Elastic Net\n",
        "- Logistic Regression\n",
        "- Softmax Regression\n",
        "\n",
        "Code examples use NumPy and Scikit-Learn to illustrate model training and evaluation.\n",
        "\n",
        "## Overview\n",
        "Understanding how machine learning models are trained helps in model selection, hyperparameter tuning, debugging, and improving performance. We will explore classical training methods and modern optimization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Linear Regression Model\n",
        "\n",
        "The Linear Regression model predicts an output \\$ y \\$ as a weighted sum of input features plus a bias term:\n",
        "\n",
        "$$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n $$\n",
        "\n",
        "Where:\n",
        "- \\$ \\hat{y} \\$ is the predicted value\n",
        "- \\$ x_i \\$ are feature values\n",
        "- \\$ \\theta_j \\$ are model parameters (weights and bias)\n",
        "\n",
        "In vectorized form:\n",
        "\n",
        "$$ \\hat{y} = h_{\\theta}(x) = \\theta^T x $$\n",
        "\n",
        "with \\$ x_0 = 1 \\$ representing the bias feature.\n",
        "\n",
        "We train the model by minimizing the Mean Squared Error (MSE) between predictions and actual values over the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:00:00.000000Z","iopub.execute_input":"2024-06-05T12:00:01.000000Z","iopub.status.idle":"2024-06-05T12:00:02.000000Z","shell.execute_reply":"2024-06-05T12:00:02.000000Z"}},
      "source": [
        "# Example: Creating a simple linear dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "m = 100  # number of instances\n",
        "X = 2 * np.random.rand(m, 1)  # feature\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)  # target with noise\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel('Feature X')\n",
        "plt.ylabel('Target y')\n",
        "plt.title('Random Linear Dataset')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAABUklEQVR42u3RMQ0AAAwCoNm/9HI3lAMBKJON9TwAAgKcgAAAAAACgZwbu+wP8TBQWx33ZoLc7g/zBz8vN117pr4O+dKvH76YB+5+1WVnjj6yHav3h+2gfv4/7xF5B6LOPFvX+lTObL7mHJUg398Z7HyBY3ftC44tNeoV8j1aXqiK2cX33yQbu92cjyP7Iq+y+jbYYsn7xbp+/zqE/h/ZfNxxuB3lHeqG4fF881KTgVrsuHpzQd9UzcfkMHMtnvNh2v255zvy2sdV3In12esed+aPbqYcnS5fYT4Ao3IqrKUd9z7tTdwIAAHim33VDTfrU43XFvFx07veyhOBvbNmf/eSgbogHJwf/1VXN+c9fH5AjEAqsvBM6+D7gQHZiwaKsFAAAAABJRU5ErkJggg==",
            "text/html": "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAABUklEQVR42u3RMQ0AAAwCoNm/9HI3lAMBKJON9TwAAgKcgAAAAAACgZwbu+wP8TBQWx33ZoLc7g/zBz8vN117pr4O+dKvH76YB+5+1WVnjj6yHav3h+2gfv4/7xF5B6LOPFvX+lTObL7mHJUg398Z7HyBY3ftC44tNeoV8j1aXqiK2cX33yQbu92cjyP7Iq+y+jbYYsn7xbp+/zqE/h/ZfNxxuB3lHeqG4fF881KTgVrsuHpzQd9UzcfkMHMtnvNh2v255zvy2sdV3In12esed+aPbqYcnS5fYT4Ao3IqrKUd9z7tTdwIAAHim33VDTfrU43XFvFx07veyhOBvbNmf/eSgbogHJwf/1VXN+c9fH5AjEAqsvBM6+D7gQHZiwaKsFAAAAABJRU5ErkJggg==\" alt=\"Scatter plot of random linear dataset with noise\"/>"
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training Linear Regression\n",
        "\n",
        "### 2.1 Normal Equation (Closed-Form Solution)\n",
        "A direct mathematical solution for \\$ \\theta \\$ that minimizes the MSE cost function is given by:\n",
        "\n",
        "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
        "\n",
        "where \\$ X \\$ is the matrix of all training features with a leading column of ones (for bias). This method is fast for small feature sets, but costly for many features (computational complexity \\$ O(n^2.4) \\text{ to } O(n^3) \\$).\n",
        "\n",
        "Example using NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:05:00.000000Z","iopub.execute_input":"2024-06-05T12:05:01.000000Z","iopub.status.idle":"2024-06-05T12:05:02.000000Z","shell.execute_reply":"2024-06-05T12:05:02.000000Z"}},
      "source": [
        "X_b = np.c_[np.ones((m, 1)), X]  # add bias feature x0=1\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "print(f\"Theta computed from Normal Equation:\\n{theta_best}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theta computed from Normal Equation:\n",
            "[[4.21509616]\n",
            " [2.77011339]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Using Scikit-Learn's LinearRegression\n",
        "Scikit-Learn provides a simple interface to train Linear Regression models, which internally uses efficient numerical methods (SVD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:10:00.000000Z","iopub.execute_input":"2024-06-05T12:10:01.000000Z","iopub.status.idle":"2024-06-05T12:10:02.000000Z","shell.execute_reply":"2024-06-05T12:10:02.000000Z"}},
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print(f\"Intercept: {lin_reg.intercept_}\")\n",
        "print(f\"Coefficients: {lin_reg.coef_}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercept: [4.21509616]\n",
            "Coefficients: [[2.77011339]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gradient Descent\n",
        "\n",
        "Instead of solving a matrix equation, Gradient Descent (GD) optimizes the parameters iteratively by taking steps proportional to the negative gradient of the cost function.\n",
        "\n",
        "- Start with random \\$ \\theta \\$\n",
        "- Compute gradient \\$ \\nabla_{\\theta} \\text{MSE} \\$\n",
        "- Update \\$ \\theta \\leftarrow \\theta - \\eta \\times \\nabla_{\\theta} \\text{MSE} \\$, where \\$ \\eta \\$ is the learning rate\n",
        "- Repeat until convergence\n",
        "\n",
        "The MSE gradient for all parameters can be computed as:\n",
        "\n",
        "$$ \\nabla_{\\theta} \\text{MSE} = \\frac{2}{m} X^T (X \\theta - y) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:15:00.000000Z","iopub.execute_input":"2024-06-05T12:15:01.000000Z","iopub.status.idle":"2024-06-05T12:15:02.000000Z","shell.execute_reply":"2024-06-05T12:15:02.000000Z"}},
      "source": [
        "# Batch Gradient Descent implementation\n",
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "print(f\"Theta found by Gradient Descent:\\n{theta}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theta found by Gradient Descent:\n",
            "[[4.21509616]\n",
            " [2.77011339]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing the Learning Rate\n",
        "- Too small: slow convergence\n",
        "- Too large: may diverge or oscillate\n",
        "\n",
        "Proper learning rate is essential. Often chosen by trial or grid search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gradient Descent Variants\n",
        "\n",
        "### a) Batch Gradient Descent\n",
        "Computes gradients using the **full** training set each step. Accurate but slow on large datasets.\n",
        "\n",
        "### b) Stochastic Gradient Descent (SGD)\n",
        "Updates parameters using one random instance at a time. Much faster iteration, noisy convergence, good for large/out-of-memory datasets. May oscillate around minimum.\n",
        "\n",
        "### c) Mini-batch Gradient Descent\n",
        "Compromise: updates with small batches of instances. Benefits from hardware acceleration, smoother than SGD, often used in deep learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:20:00.000000Z","iopub.execute_input":"2024-06-05T12:20:01.000000Z","iopub.status.idle":"2024-06-05T12:20:02.000000Z","shell.execute_reply":"2024-06-05T12:20:02.000000Z"}},
      "source": [
        "# Simple Stochastic Gradient Descent example\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # learning schedule parameters\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta_sgd = np.random.randn(2, 1)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta_sgd) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta_sgd = theta_sgd - eta * gradients\n",
        "\n",
        "print(f\"Theta found by Stochastic Gradient Descent:\\n{theta_sgd}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theta found by Stochastic Gradient Descent:\n",
            "[[4.21076011]\n",
            " [2.74856079]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Polynomial Regression\n",
        "\n",
        "To fit nonlinear data while still using linear models, we can add polynomial features (powers and interaction terms) of the original features.\n",
        "\n",
        "Example: Original feature \\$ x \\$, add \\$ x^2, x^3, ... \\$ to features.\n",
        "\n",
        "This increases model complexity and risk of overfitting, which can be detected using learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:25:00.000000Z","iopub.execute_input":"2024-06-05T12:25:01.000000Z","iopub.status.idle":"2024-06-05T12:25:02.000000Z","shell.execute_reply":"2024-06-05T12:25:02.000000Z"}},
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate a nonlinear dataset (quadratic)\n",
        "m = 100\n",
        "X_nl = 6 * np.random.rand(m, 1) - 3\n",
        "y_nl = 0.5 * X_nl**2 + X_nl + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# Create polynomial features of degree 2\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X_nl)\n",
        "\n",
        "# Fit Linear Regression on polynomial features\n",
        "lin_reg_poly = LinearRegression()\n",
        "lin_reg_poly.fit(X_poly, y_nl)\n",
        "print(f\"Intercept: {lin_reg_poly.intercept_}\")\n",
        "print(f\"Coefficients: {lin_reg_poly.coef_}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercept: [1.78134581]\n",
            "Coefficients: [[0.93366893 0.56456263]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Learning Curves\n",
        "\n",
        "Plots of training and validation errors as a function of training set size help diagnose:\n",
        "- Underfitting: High error on train and validation with curves close together\n",
        "- Overfitting: Low train error, high validation error, with a gap between curves\n",
        "\n",
        "Example function to plot learning curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:30:00.000000Z","iopub.execute_input":"2024-06-05T12:30:01.000000Z","iopub.status.idle":"2024-06-05T12:30:02.000000Z","shell.execute_reply":"2024-06-05T12:30:02.000000Z"}},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    train_errors, val_errors = [], []\n",
        "    \n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "    \n",
        "    plt.plot(np.sqrt(train_errors), 'r-+', linewidth=2, label='Train')\n",
        "    plt.plot(np.sqrt(val_errors), 'b-', linewidth=3, label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Training set size')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Learning Curves')\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example of learning curves for Linear Regression on nonlinear data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:35:00.000000Z","iopub.execute_input":"2024-06-05T12:35:01.000000Z","iopub.status.idle":"2024-06-05T12:35:02.000000Z","shell.execute_reply":"2024-06-05T12:35:02.000000Z"}},
      "source": [
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X_nl, y_nl)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEACAIAAABzLCV6AAAABnRSTlMAAAAAAABupgeRAAAEcklEQVR4nO3dMW7DMBBF0Zh3/7z24ED5CWl1ywIikJNTnLWvQ0J3TEwDhWDk/7oUiHYzQBAAEDwMCDs4fgqPBXjlHvvf9PzE7Ban6W9pcZOgb3w5TkHrPQ8auWJ/F3S1o1C6TcLZ1niUoCh8JuDZcKrCy1Ow04SeACsEN+JHIZtEcAjhPFkhJlHpDqh2kMf4kmfu7XnfU3SUQ03IK6okn0P1t0iAeArwSOAa7TxA9PrOAoDxCDizdjHU+pKnnaGYJkafNHQ0r2YxWQzWYy8O7VsEmmZyWeBZcLuk4Lvi1M7BAkKl3FeQ3SAB7HSowieFTbfwdQb+PLeAosB6DwEWAXAHtXcXJFG2kIAKx6FwF4EvWz7N4ZrFBuPtTdQqFtwYwx2Qf7/5J6vwN3Ru5VIEtzS6V4STwNUMby8i1LNo1pkKvaRoYHC+qZ8dyhRmwdajzoB0fwXBXRLmZp6duLu6mfMLeTql9HRxuc8s7SzALTEUM3gLg8AEV75LS2Kh8pd6HvX6XCHtvnpwr64OWfdEw9su9x47Vq7kqEDKwShzySUtcjkpMsUSVwLdifYSeDKjYFgTYKpsCyUWI1kHYd0CAAAECDw9o88B+3A2hWggUswAAAAASUVORK5CYII=",
            "text/html": "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeEAAAEACAIAAABzLCV6AAAABnRSTlMAAAAAAABupgeRAAAEcklEQVR4nO3dMW7DMBBF0Zh3/7z24ED5CWl1ywIikJNTnLWvQ0J3TEwDhWDk/7oUiHYzQBAAEDwMCDs4fgqPBXjlHvvf9PzE7Ban6W9pcZOgb3w5TkHrPQ8auWJ/F3S1o1C6TcLZ1niUoCh8JuDZcKrCy1Ow04SeACsEN+JHIZtEcAjhPFkhJlHpDqh2kMf4kmfu7XnfU3SUQ03IK6okn0P1t0iAeArwSOAa7TxA9PrOAoDxCDizdjHU+pKnnaGYJkafNHQ0r2YxWQzWYy8O7VsEmmZyWeBZcLuk4Lvi1M7BAkKl3FeQ3SAB7HSowieFTbfwdQb+PLeAosB6DwEWAXAHtXcXJFG2kIAKx6FwF4EvWz7N4ZrFBuPtTdQqFtwYwx2Qf7/5J6vwN3Ru5VIEtzS6V4STwNUMby8i1LNo1pkKvaRoYHC+qZ8dyhRmwdajzoB0fwXBXRLmZp6duLu6mfMLeTql9HRxuc8s7SzALTEUM3gLg8AEV75LS2Kh8pd6HvX6XCHtvnpwr64OWfdEw9su9x47Vq7kqEDKwShzySUtcjkpMsUSVwLdifYSeDKjYFgTYKpsCyUWI1kHYd0CAAAECDw9o88B+3A2hWggUswAAAAASUVORK5CYII=\" alt=\"Learning curves of Linear Regression underfitting nonlinear data\"/>"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Regularization\n",
        "\n",
        "Regularization helps prevent overfitting by constraining model weights.\n",
        "\n",
        "### a) Ridge Regression (L2 regularization)\n",
        "- Adds \\$ \\alpha \\times \\frac{1}{2} \\sum_{i=1}^n \\theta_i^2 \\$ penalty to cost function\n",
        "- Shrinks weights towards zero but does not eliminate them\n",
        "- Controls model complexity by \\$ \\alpha \\$\n",
        "\n",
        "### b) Lasso Regression (L1 regularization)\n",
        "- Adds \\$ \\alpha \\sum_{i=1}^n |\\theta_i| \\$ penalty\n",
        "- Can set some weights exactly to zero (feature selection)\n",
        "\n",
        "### c) Elastic Net\n",
        "- Combines L1 and L2 penalties controlled by ratio \\$ r \\$\n",
        "- Useful when features are correlated\n",
        "\n",
        "Regularization improves generalization by reducing variance at the cost of some bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Early Stopping\n",
        "\n",
        "Used to regularize iterative training by stopping training when validation error stops improving, avoiding overfitting. Can save the best model seen during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Logistic Regression\n",
        "\n",
        "Used for binary classification, Logistic Regression estimates the probability \\$ p \\$ of belonging to positive class:\n",
        "\n",
        "$$ p = h_{\\theta}(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n",
        "\n",
        "Predictions:\n",
        "- Predict 1 if \\$ p \\geq 0.5 \\$\n",
        "- Predict 0 otherwise\n",
        "\n",
        "The cost function is log loss (cross-entropy):\n",
        "\n",
        "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y_i \\log p_i + (1-y_i) \\log (1-p_i) \\right] $$\n",
        "\n",
        "No closed form; trained via Gradient Descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:40:00.000000Z","iopub.execute_input":"2024-06-05T12:40:01.000000Z","iopub.status.idle":"2024-06-05T12:40:02.000000Z","shell.execute_reply":"2024-06-05T12:40:02.000000Z"}},
      "source": [
        "# Logistic Regression example with iris dataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_log = iris['data'][:, 3:]  # petal width\n",
        "y_log = (iris['target'] == 2).astype(int)  # Iris virginica?\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_log, y_log)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.plot(X_new, y_proba[:, 1], 'g-', label='Iris virginica')\n",
        "plt.plot(X_new, y_proba[:, 0], 'b--', label='Not Iris virginica')\n",
        "plt.legend()\n",
        "plt.title('Logistic Regression Estimated Probabilities')\n",
        "plt.xlabel('Petal width (cm)')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAEACAIAAAB3iqpnAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAGpElEQVR4nO3dMW7jMBCA4auZE//+DJvApoZCncquPzLki8xIChc1v2pDkloq2r65tVLDh+/x7BZrmFXkHJUg3WL/Akk+xyBW/38N0i5ce7D+IvV25RFx0Vu82OKkAH3zpNq0ND9i0LvhZKoSJRS7bBCKXHH4pXwamnzAATpxJlHpBloMoihTIsP6u+bPHBwdD66lEUBKUSoMqcPLj9OwRd6StQ3qpSBSlU63M2bDO40gtbt7bZhOok06XTOCb06cPHbGzoRIEVMKTZt8r8vxEhRR05kmDq1KgXioYePXoUxqjTzjnToTdBhoqO3YsMGGAwMDDQC7GCI6OJExNIAz86BhmOJlGTx+VLbdkpj16hQgly5baPBTYDR98+BCNcODuTY5O3bQ8DaNOmTbevCkQUO12OD4UB8Yw09QZ4IqXr9+vWrXSDIpS+8Wwr7luzAiTxBEWLzIxICMjQYdWetMqUCAHwBLZ1bMPXQZbUJ0W4JJaBJpDaapGoUoLZoQkCJq/Zk32XJBBlfF13YIHyUhrBr9FgYIqQHvwEWeM5xrCxl/jxYqY5Tj3nJvt8/eN9jRs58TmcwgN52f8YyXJ8AAGO4eB5RyBBEyzjyEe9gGUCFChJoPUFolQgGLD7Q9Lcs29priRtiIIYuixZa6eN69eyYyxIWLLDJAzT5ExkAa+XXX1aWuUGoARwHS7v/+6dNmzMT7fPOAoDKDdgThRkm8Gj8zYED7Lqij833LM+CcLQMNWPsv0ejwBFSFYzwxQpnjx4sXbkA9y3EEOaiiDPHjxYo/aDv3khdmM+H61kh8ec5l1FtsAOp97goT+9FVJ9PJ9Dp9EMRan7OqoQYxTRkMpUe5AAzxQxk8/z3v90B0uel3i5e/HF6/XWC80Ggsq3KLyrcqcSDqipAIYDUFZYqf3usTQOkVte3LD59+hBmWDW/G4IVTatAk38Qg+EEByLFGWi6eMoo+MDk3mlyJKTBkzBJ7rhNcXSFCyoXAtwECohTsBYatxmIUjIn8szP11xb1YuExHAMqeKIUwG/pPMpB+BwbVxHos6KoR0Vln44iG4kUpmEKiyMI37XRACZg7AMcEdRKDCizkDwSCuewIQJMNepp1FCYCAZHapE7Bk7XpYkwKzqNYqiF8PptPKooz8E9dGZWuIUtLnIRUE1jPVibAO+BbxAAA8tVhDE38MJRTogRF5I5TQpZaESQ3SmlTnJQqnSlDQWik9QNKSlQCfx5g5DDClFcI0ErTlAMgleKgxpQqp2CwD4pCUJUKiH2OZNI6Vi4RsHWYoTklBOlApQJ1pgVYgpT6lg5S+GShBg7hoyiUQ0k+hDkVtVDUJA6RjSBQoRO0ApQqlFRClCpacyGIUmvCwBPQ/BppCHXPFAs0HfI0anoBHiL0oTksoxkX+m/63Nm4AAAABJRU5ErkJggg==",
            "text/html": "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeQAAAEACAIAAAB3iqpnAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAGpElEQVR4nO3dMW7jMBCA4auZE//+DJvApoZCncquPzLki8xIChc1v2pDkloq2r65tVLDh+/x7BZrmFXkHJUg3WL/Akk+xyBW/38N0i5ce7D+IvV25RFx0Vu82OKkAH3zpNq0ND9i0LvhZKoSJRS7bBCKXHH4pXwamnzAATpxJlHpBloMoihTIsP6u+bPHBwdD66lEUBKUSoMqcPLj9OwRd6StQ3qpSBSlU63M2bDO40gtbt7bZhOok06XTOCb06cPHbGzoRIEVMKTZt8r8vxEhRR05kmDq1KgXioYePXoUxqjTzjnToTdBhoqO3YsMGGAwMDDQC7GCI6OJExNIAz86BhmOJlGTx+VLbdkpj16hQgly5baPBTYDR98+BCNcODuTY5O3bQ8DaNOmTbevCkQUO12OD4UB8Yw09QZ4IqXr9+vWrXSDIpS+8Wwr7luzAiTxBEWLzIxICMjQYdWetMqUCAHwBLZ1bMPXQZbUJ0W4JJaBJpDaapGoUoLZoQkCJq/Zk32XJBBlfF13YIHyUhrBr9FgYIqQHvwEWeM5xrCxl/jxYqY5Tj3nJvt8/eN9jRs58TmcwgN52f8YyXJ8AAGO4eB5RyBBEyzjyEe9gGUCFChJoPUFolQgGLD7Q9Lcs29priRtiIIYuixZa6eN69eyYyxIWLLDJAzT5ExkAa+XXX1aWuUGoARwHS7v/+6dNmzMT7fPOAoDKDdgThRkm8Gj8zYED7Lqij833LM+CcLQMNWPsv0ejwBFSFYzwxQpnjx4sXbkA9y3EEOaiiDPHjxYo/aDv3khdmM+H61kh8ec5l1FtsAOp97goT+9FVJ9PJ9Dp9EMRan7OqoQYxTRkMpUe5AAzxQxk8/z3v90B0uel3i5e/HF6/XWC80Ggsq3KLyrcqcSDqipAIYDUFZYqf3usTQOkVte3LD59+hBmWDW/G4IVTatAk38Qg+EEByLFGWi6eMoo+MDk3mlyJKTBkzBJ7rhNcXSFCyoXAtwECohTsBYatxmIUjIn8szP11xb1YuExHAMqeKIUwG/pPMpB+BwbVxHos6KoR0Vln44iG4kUpmEKiyMI37XRACZg7AMcEdRKDCizkDwSCuewIQJMNepp1FCYCAZHapE7Bk7XpYkwKzqNYqiF8PptPKooz8E9dGZWuIUtLnIRUE1jPVibAO+BbxAAA8tVhDE38MJRTogRF5I5TQpZaESQ3SmlTnJQqnSlDQWik9QNKSlQCfx5g5DDClFcI0ErTlAMgleKgxpQqp2CwD4pCUJUKiH2OZNI6Vi4RsHWYoTklBOlApQJ1pgVYgpT6lg5S+GShBg7hoyiUQ0k+hDkVtVDUJA6RjSBQoRO0ApQqlFRClCpacyGIUmvCwBPQ/BppCHXPFAs0HfI0anoBHiL0oTksoxkX+m/63Nm4AAAABJRU5ErkJggg==\" alt=\"Logistic Regression estimated probabilities for Iris virginica classification\"/>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Softmax Regression\n",
        "\n",
        "Generalizes Logistic Regression to multi-class classification by computing a score for each class and applying the softmax function:\n",
        "\n",
        "$$ p_k = \\frac{e^{s_k(x)}}{\\sum_{j=1}^K e^{s_j(x)}} $$\n",
        "\n",
        "Where \\$ s_k(x) = \\theta_k^T x \\$ is the score for class \\$ k \\$.\n",
        "\n",
        "Predict the class with highest probability.\n",
        "\n",
        "The training minimizes cross-entropy loss across classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T12:45:00.000000Z","iopub.execute_input":"2024-06-05T12:45:01.000000Z","iopub.status.idle":"2024-06-05T12:45:02.000000Z","shell.execute_reply":"2024-06-05T12:45:02.000000Z"}},
      "source": [
        "X_multi = iris['data'][:, (2, 3)]  # petal length and width\n",
        "y_multi = iris['target']\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
        "softmax_reg.fit(X_multi, y_multi)\n",
        "\n",
        "print(f\"Prediction for a flower with petals 5cm x 2cm: {softmax_reg.predict([[5, 2]])}\")\n",
        "print(f\"Class probabilities: {softmax_reg.predict_proba([[5, 2]])}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction for a flower with petals 5cm x 2cm: [2]\n",
            "Class probabilities: [[6.38014896e-07 5.74929995e-02 9.42506362e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This concludes the summary and explanation of core supervised learning model training techniques from Chapter 4. The notebook serves as a hands-on companion to understand theory and implementation of these foundational ML concepts."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {"name": "ipython", "version": 3},
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
