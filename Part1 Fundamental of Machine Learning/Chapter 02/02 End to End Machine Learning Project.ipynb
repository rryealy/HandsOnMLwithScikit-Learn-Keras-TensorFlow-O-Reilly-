{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7636b0ec",
   "metadata": {},
   "source": [
    "# üè† End-to-End Machine Learning Project\n",
    "**Source: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (Chapter 2)**\n",
    "\n",
    "This notebook demonstrates a complete ML workflow to predict housing prices in California using the `housing.csv` dataset. It includes:\n",
    "- Data download and loading\n",
    "- Exploratory data analysis\n",
    "- Data preprocessing (handling missing values, feature engineering, encoding)\n",
    "- Train/test split with stratification\n",
    "- Pipeline building using Scikit-Learn\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf989ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1f1b2",
   "metadata": {},
   "source": [
    "**Theory**: In Machine Learning projects, reproducibility is important. Therefore, automating the data retrieval and loading process allows for consistent and repeatable experiments. Here, we define functions to download and load the California housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f6ca4",
   "metadata": {},
   "source": [
    "## üì• Download and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    with tarfile.open(tgz_path) as housing_tgz:\n",
    "        housing_tgz.extractall(path=housing_path)\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eb2016",
   "metadata": {},
   "source": [
    "**Theory**: Initial exploration helps us understand the data‚Äôs shape, type, and quality. We use `info()`, `describe()`, and histograms to detect missing values, data types, value ranges, and possible outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a0e2c",
   "metadata": {},
   "source": [
    "## üîé Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb01551",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22ab38",
   "metadata": {},
   "source": [
    "**Theory**: Creating a stratified split ensures that the train and test sets are representative of the overall dataset, especially for important features like `median_income`. This prevents sampling bias and helps the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb517ac",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Create Stratified Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ba4fe",
   "metadata": {},
   "source": [
    "**Theory**: Visualizing data geospatially (e.g., plotting longitude vs. latitude) allows us to spot regional patterns that may not be obvious in tabular form. Coloring by target variable and scaling by population size makes spatial trends more visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88126135",
   "metadata": {},
   "source": [
    "## üß≠ Geographical Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c5d1c",
   "metadata": {},
   "source": [
    "**Theory**: Pearson correlation coefficient measures linear relationships between numerical features. Feature engineering involves creating new features (e.g., `rooms_per_household`) to better capture the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07222b63",
   "metadata": {},
   "source": [
    "## üìä Correlation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6887cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]\n",
    "\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f57d2",
   "metadata": {},
   "source": [
    "**Theory**: A data preprocessing pipeline helps automate and structure the transformation of raw data. Scikit-Learn pipelines allow chaining of preprocessing steps like imputation, feature scaling, and encoding. This ensures consistency between training and prediction phases and supports hyperparameter tuning more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83906c38",
   "metadata": {},
   "source": [
    "## üßπ Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eee3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels = housing[\"median_house_value\"].copy()\n",
    "housing = housing.drop(\"median_house_value\", axis=1)\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, 3] / X[:, 6]\n",
    "        population_per_household = X[:, 5] / X[:, 6]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, 4] / X[:, 3]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
