{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machines (SVM) - Summary and Practical Examples\n",
        "\n",
        "This notebook summarizes key theoretical concepts from the chapter on Support Vector Machines (SVM), including:\n",
        "- Linear SVM classification (hard margin, soft margin)\n",
        "- Kernel trick and nonlinear SVM classification\n",
        "- SVM regression\n",
        "- Online SVMs and hinge loss\n",
        "\n",
        "Practical examples use Scikit-Learn to illustrate core concepts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Support Vector Machines\n",
        "\n",
        "SVM is a powerful model for classification, regression, and outlier detection. It seeks the widest possible margin (\"street\") separating classes by maximizing the distance between the decision boundary and the closest training points called **support vectors**. This large margin principle improves generalization.\n",
        "\n",
        "**Key idea:** Fit a hyperplane that separates classes while maximizing the margin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Linear SVM Classification\n",
        "\n",
        "- **Hard margin:** strict separation, no errors allowed; works only if data is perfectly linearly separable and sensitive to outliers.\n",
        "- **Soft margin:** allows some margin violations controlled by hyperparameter \\$ C \\$, trading off margin size and classification errors for better generalization.\n",
        "\n",
        "### SVM Objective (Soft Margin):\n",
        "\\$ \\min_{w,b,\\zeta} \\; \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^m \\zeta_i \\quad \\text{subject to} \\quad y_i(w^T x_i + b) \\geq 1 - \\zeta_i, \\; \\zeta_i \\geq 0 \\$\n",
        "\n",
        "### SVM prediction function for a new instance \\$x\\$:\n",
        "\\$ f(x) = \\text{sign}(w^T x + b) \\$\n",
        "\n",
        "Training finds \\$w, b\\$ to maximize margin while minimizing violations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:00:00.000000Z","iopub.execute_input":"2024-06-05T13:00:01.000000Z","iopub.status.idle":"2024-06-05T13:00:02.000000Z","shell.execute_reply":"2024-06-05T13:00:02.000000Z"}},
      "source": [
        "# Linear SVM classification example on iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # petal length and width\n",
        "y = (iris.target == 2).astype(np.float64)  # Iris virginica vs others\n",
        "\n",
        "svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
        "])\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "# Predict a sample\n",
        "print(svm_clf.predict([[5.5, 1.7]]))  # expected: 1 (Iris virginica)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Scaling Importance\n",
        "\n",
        "SVMs are sensitive to feature scales. Always standardize features (zero mean, unit variance) to get better margins and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Nonlinear SVM Classification and Kernel Trick\n",
        "\n",
        "Most datasets are not linearly separable. Two approaches:\n",
        "- Manually add polynomial features to make data linearly separable (costly for high degrees)\n",
        "- Use kernel trick to implicitly compute dot products in high-dimensional space without explicit feature mapping.\n",
        "\n",
        "Common kernels:\n",
        "- Linear kernel: \\$ K(a,b) = a^T b \\$\n",
        "- Polynomial kernel: \\$ K(a,b) = (\\gamma a^T b + r)^d \\$\n",
        "- Gaussian RBF kernel: \\$ K(a,b) = \\exp(-\\gamma \\|a - b\\|^2) \\$\n",
        "- Sigmoid kernel: \\$ K(a,b) = \\tanh(\\gamma a^T b + r) \\$\n",
        "\n",
        "Kernel trick lets SVMs learn complex decision boundaries efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:05:00.000000Z","iopub.execute_input":"2024-06-05T13:05:01.000000Z","iopub.status.idle":"2024-06-05T13:05:02.000000Z","shell.execute_reply":"2024-06-05T13:05:02.000000Z"}},
      "source": [
        "# Nonlinear SVM with polynomial kernel on moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "])\n",
        "poly_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "print('Support vectors count:', len(poly_kernel_svm_clf.named_steps[\"svm_clf\"].support_))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Support vectors count: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gaussian RBF Kernel\n",
        "\n",
        "The Gaussian RBF kernel maps instances to an infinite-dimensional space allowing SVMs to learn very complex boundaries with only a few hyperparameters to tune: \\$ \\gamma \\$ controlling influence of instances and \\$ C \\$ controlling trade-off between margin width and violations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:10:00.000000Z","iopub.execute_input":"2024-06-05T13:10:01.000000Z","iopub.status.idle":"2024-06-05T13:10:02.000000Z","shell.execute_reply":"2024-06-05T13:10:02.000000Z"}},
      "source": [
        "# SVM with RBF kernel on moons dataset\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=2, C=1))\n",
        "])\n",
        "rbf_kernel_svm_clf.fit(X, y)\n",
        "print('Support vectors count:', len(rbf_kernel_svm_clf.named_steps[\"svm_clf\"].support_))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Support vectors count: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dual Problem and Kernel Trick\n",
        "\n",
        "SVM training can be expressed as a dual quadratic optimization problem focusing on the support vectors and dot products between input vectors.\n",
        "\n",
        "By substituting dot products with kernel functions, the \"kernel trick\" allows learning in high-dimensional feature spaces without explicit transformation.\n",
        "\n",
        "This reduces computation and enables effective nonlinear classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. SVM Regression (Support Vector Regression)\n",
        "\n",
        "SVMs can also perform regression by fitting a \"tube\" of width \\$ \\epsilon \\$ around the regression function, ignoring errors within the tube and penalizing ones outside it.\n",
        "\n",
        "Linear and kernelized versions exist similar to classification.\n",
        "\n",
        "Example using linear SVR in Scikit-Learn follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:15:00.000000Z","iopub.execute_input":"2024-06-05T13:15:01.000000Z","iopub.status.idle":"2024-06-05T13:15:02.000000Z","shell.execute_reply":"2024-06-05T13:15:02.000000Z"}},
      "source": [
        "# Linear Support Vector Regression example\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "# Generate linear data with noise\n",
        "X_reg = 2 * np.random.rand(100, 1)\n",
        "y_reg = 4 + 3 * X_reg.ravel() + np.random.randn(100)\n",
        "\n",
        "svr_reg = LinearSVR(epsilon=0.5, C=1.0, max_iter=10000)\n",
        "svr_reg.fit(X_reg, y_reg)\n",
        "print(f\"SVR coefficient: {svr_reg.coef_}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVR coefficient: [3.01087415]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Online SVMs and Hinge Loss\n",
        "\n",
        "SVMs can be trained incrementally using stochastic gradient descent on the hinge loss function:\n",
        "\n",
        "\\$ J(w,b) = \\frac{1}{2} \\|w\\|^2 + C \\sum_i \\max(0, 1 - y_i(w^T x_i + b)) \\$\n",
        "\n",
        "The hinge loss penalizes instances on the wrong side or inside the margin and is linear beyond the margin threshold.\n",
        "\n",
        "Online training is useful for large-scale or streaming data.\n",
        "\n",
        "For very large nonlinear problems, deeper models like neural networks may be preferable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This notebook provides a practical and theoretical overview of SVM to get started with training and using Support Vector Machines effectively."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
