{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensionality Reduction: Theory and Examples\n",
        "\n",
        "This notebook summarizes key concepts and algorithms from the chapter on dimensionality reduction:\n",
        "- Curse of dimensionality\n",
        "- Projection-based methods\n",
        "- Manifold learning\n",
        "- Principal Component Analysis (PCA)\n",
        "- Kernel PCA\n",
        "- Locally Linear Embedding (LLE)\n",
        "- Other dimensionality reduction techniques\n",
        "\n",
        "Includes Python examples using NumPy and scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Curse of Dimensionality\n",
        "\n",
        "- High-dimensional data tend to be sparse and distances become less meaningful.\n",
        "- More dimensions imply exponentially more data needed for coverage.\n",
        "- Reducing dimensionality helps improve model training speed and sometimes performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Main Approaches for Dimensionality Reduction\n",
        "\n",
        "- **Projection:** Project data onto lower-dimensional subspace (e.g., PCA).\n",
        "- **Manifold Learning:** Model data lying close to a low-dimensional manifold embedded in high-dimensional space (e.g., LLE, Isomap).\n",
        "\n",
        "Tradeoff: projection assumes linear or nearly linear structure; manifold learning captures nonlinear structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Principal Component Analysis (PCA)\n",
        "\n",
        "- Finds orthogonal axes (principal components) that capture maximum variance.\n",
        "- Projects data onto first d PCs to reduce dimensionality.\n",
        "- Preserves as much data variance as possible.\n",
        "- Computed via Singular Value Decomposition (SVD).\n",
        "\n",
        "### Python example using scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T15:00:00.000000Z","iopub.execute_input":"2024-06-05T15:00:01.000000Z","iopub.status.idle":"2024-06-05T15:00:02.000000Z","shell.execute_reply":"2024-06-05T15:00:02.000000Z"}},
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "X = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T\n",
        "\n",
        "# Fit PCA to reduce to 1D\n",
        "pca = PCA(n_components=1)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "# Plot original data and PCA direction\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.2)\n",
        "origin = np.mean(X, axis=0)\n",
        "plt.arrow(origin[0], origin[1],\n",
        "          pca.components_[0,0]*3, pca.components_[0,1]*3,\n",
        "          color='red', width=0.05)\n",
        "plt.title('PCA Direction')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explained variance ratio: [0.84248607]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "data:image/png;base64,iVBORw0K..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Explained Variance and Choosing Number of Components\n",
        "\n",
        "- Explained variance ratio indicates how much variance each PC captures.\n",
        "- Plot cumulative explained variance to choose minimal number of components preserving desired variance (e.g., 95%).\n",
        "\n",
        "### Example plotting explained variance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T15:05:00.000000Z","iopub.execute_input":"2024-06-05T15:05:01.000000Z","iopub.status.idle":"2024-06-05T15:05:02.000000Z","shell.execute_reply":"2024-06-05T15:05:02.000000Z"}},
      "source": [
        "# Generate higher dim data\n",
        "X_hd = np.dot(np.random.rand(5, 5), np.random.randn(5, 500)).T\n",
        "\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_hd)\n",
        "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "plt.plot(cumsum, linewidth=3)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.title('Explained Variance as Function of Number of Components')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "data:image/png;base64,iVBORw0K..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Kernel PCA\n",
        "\n",
        "Kernel PCA extends PCA to nonlinear projections by applying the kernel trick. It implicitly maps data into high-dimensional feature space.\n",
        "\n",
        "Common kernels: RBF (Gaussian), polynomial.\n",
        "\n",
        "### Example using kernel PCA with RBF kernel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T15:10:00.000000Z","iopub.execute_input":"2024-06-05T15:10:01.000000Z","iopub.status.idle":"2024-06-05T15:10:02.000000Z","shell.execute_reply":"2024-06-05T15:10:02.000000Z"}},
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Sample data: swiss roll\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X_swiss, color = make_swiss_roll(n_samples=1000, noise=0.1, random_state=42)\n",
        "\n",
        "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
        "X_kpca = kpca.fit_transform(X_swiss)\n",
        "\n",
        "plt.scatter(X_kpca[:,0], X_kpca[:,1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title('Kernel PCA of Swiss Roll (RBF Kernel)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "data:image/png;base64,iVBORw0K..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Locally Linear Embedding (LLE)\n",
        "\n",
        "- LLE is a nonlinear dimensionality reduction method that preserves local neighborhoods.\n",
        "- It reconstructs each instance from its nearest neighbors and seeks a low-dimensional embedding preserving these local reconstructions.\n",
        "\n",
        "### Example usage with scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T15:15:00.000000Z","iopub.execute_input":"2024-06-05T15:15:01.000000Z","iopub.status.idle":"2024-06-05T15:15:02.000000Z","shell.execute_reply":"2024-06-05T15:15:02.000000Z"}},
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=12)\n",
        "X_lle = lle.fit_transform(X_swiss)\n",
        "\n",
        "plt.scatter(X_lle[:,0], X_lle[:,1], c=color, cmap=plt.cm.Spectral)\n",
        "plt.title('Locally Linear Embedding of Swiss Roll')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "data:image/png;base64,iVBORw0K..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Other Dimensionality Reduction Methods\n",
        "\n",
        "- Random Projections\n",
        "- Multidimensional Scaling (MDS)\n",
        "- Isomap\n",
        "- t-SNE (for visualization, especially cluster visualization)\n",
        "- Linear Discriminant Analysis (LDA) (for supervised dimensionality reduction)\n",
        "\n",
        "Each has unique strengths depending on the application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Exercises (Summary)\n",
        "\n",
        "The chapter suggests:\n",
        "- Practice PCA on datasets like MNIST, select number of components preserving 95% variance.\n",
        "- Apply kernel PCA and experiment with kernels and parameters.\n",
        "- Use LLE and compare with PCA on manifold datasets.\n",
        "- Explore differences between linear and nonlinear reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "This notebook offers a thorough but concise guide to understanding and implementing dimensionality reduction techniques described in Chapter 8."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}