{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees: Theory and Practice\n",
        "\n",
        "This notebook summarizes the key concepts of Decision Trees presented in Chapter 6, including:\n",
        "- Introduction and basics\n",
        "- Training and visualization\n",
        "- Prediction mechanism\n",
        "- CART training algorithm\n",
        "- Regularization of trees\n",
        "- Decision tree regression\n",
        "- Limitations and instability\n",
        "- Exercises overview\n",
        "\n",
        "Examples use Scikit-Learn for demonstration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What are Decision Trees?\n",
        "\n",
        "Decision Trees are versatile supervised learning models for classification and regression. They split data into subsets by testing feature thresholds, forming a tree of decision rules.\n",
        "\n",
        "Advantages:\n",
        "- Easy to interpret (white box model)\n",
        "- Can handle numerical and categorical data\n",
        "- Requires little data preprocessing (no scaling needed)\n",
        "\n",
        "Disadvantages:\n",
        "- Prone to overfitting if not regularized\n",
        "- Can be unstable/sensitive to training data variations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training and Visualizing a Decision Tree\n",
        "\n",
        "Example: Training a DecisionTreeClassifier on Iris dataset with petal length and width features, max depth = 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:30:00.000000Z","iopub.execute_input":"2024-06-05T13:30:01.000000Z","iopub.status.idle":"2024-06-05T13:30:02.000000Z","shell.execute_reply":"2024-06-05T13:30:02.000000Z"}},
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:]  # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X, y)\n",
        "\n",
        "# Visualize the tree\n",
        "plt.figure(figsize=(12,8))\n",
        "plot_tree(tree_clf, feature_names=iris.feature_names[2:], class_names=iris.target_names, filled=True, rounded=True)\n",
        "plt.title('Decision Tree Trained on Iris Data (max_depth=2)')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAC2LE3PAAADc..."
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. How Prediction Works in Decision Trees\n",
        "\n",
        "Prediction starts at the root, comparing feature values against thresholds.\n",
        "- If condition is true, go left child\n",
        "- Else, go right child\n",
        "- Repeat until leaf node\n",
        "\n",
        "The leaf node predicts the class or value based on training instances falling there.\n",
        "\n",
        "Example: Petal length < 2.45 cm leads to Iris setosa class prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CART Training Algorithm\n",
        "\n",
        "CART (Classification and Regression Trees) grows binary trees by recursively splitting the training data to minimize impurity (Gini impurity or entropy).\n",
        "At each node, it finds the best feature and threshold that results in purest subsets weighted by size.\n",
        "\n",
        "Impurity measures:\n",
        "- Gini impurity (default):  $$ G_i = 1 - \\sum_k p_{i,k}^2 $$\n",
        "- Entropy:  $$ H_i = -\\sum_k p_{i,k} \\, \\log_2 p_{i,k} $$\n",
        "\n",
        "The algorithm is greedy and recursive until max depth or stopping criteria is met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Regularization Hyperparameters\n",
        "\n",
        "Decision Trees tend to overfit if unrestricted. To regularize, use hyperparameters:\n",
        "- `max_depth`: maximum tree depth\n",
        "- `min_samples_split`: minimum samples to split a node\n",
        "- `min_samples_leaf`: minimum samples in a leaf node\n",
        "- `max_leaf_nodes`: maximum number of leaf nodes\n",
        "- `max_features`: max features evaluated for each split\n",
        "\n",
        "Increasing minimums or limiting max depth/nodes controls overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:40:00.000000Z","iopub.execute_input":"2024-06-05T13:40:01.000000Z","iopub.status.idle":"2024-06-05T13:40:02.000000Z","shell.execute_reply":"2024-06-05T13:40:02.000000Z"}},
      "source": [
        "# Decision Tree with regularization example\n",
        "tree_reg_clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=42)\n",
        "tree_reg_clf.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plot_tree(tree_reg_clf, feature_names=iris.feature_names[2:], class_names=iris.target_names, filled=True, rounded=True)\n",
        "plt.title('Regularized Decision Tree (max_depth=3, min_samples_leaf=5)')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAC2LE3PAAADc..."
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Decision Trees for Regression\n",
        "\n",
        "DecisionTreeRegressor predicts continuous values by averaging target values in leaf nodes.\n",
        "\n",
        "Example: Train a Decision Tree regressor on noisy quadratic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"execution":{"iopub.status.busy":"2024-06-05T13:50:00.000000Z","iopub.execute_input":"2024-06-05T13:50:01.000000Z","iopub.status.idle":"2024-06-05T13:50:02.000000Z","shell.execute_reply":"2024-06-05T13:50:02.000000Z"}},
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "m = 200\n",
        "X_reg = np.sort(5 * np.random.rand(m, 1), axis=0)\n",
        "y_reg = np.sin(X_reg).ravel() + 0.5 * np.random.randn(m)\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(max_depth=3)\n",
        "tree_reg.fit(X_reg, y_reg)\n",
        "\n",
        "# Visualize predictions\n",
        "X_test = np.linspace(0, 5, 1000).reshape(-1, 1)\n",
        "y_pred = tree_reg.predict(X_test)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_reg, y_reg, s=20, label='Training data')\n",
        "plt.plot(X_test, y_pred, color='r', linewidth=2, label='Predictions')\n",
        "plt.title('Decision Tree Regression')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAEsCAIAAABXhZD/AAAB2klEQVR4nO3doQ2AIBAEQb3//5juCF9G2wRhvpJW6nXG79uAg0TRiYOvTnkT26XYUIAQAAEDAB4DLP5dAAczPtxCAY2SqUTvQcWGAwAACAABAIAAgAAQKgHoFTA8XmCsdgWtb9vRzRW+7WcFoN5yb6FrKYqxwNQ+pdmhREWj32rhY7qkslISCJhJ+qCIF/zBkiMjkDAQCAAgAAARZQVKcA/4AbC9NmYpAAgAAEAgAAIABD6r2QECmFUvq5gOrXpdFEooBoN3fReEQTEUG6rlyUQQAAQCAAgAAFgAB564Z7QAMAAMAAMAADIAYI8AJwAAGMAAYAAGAIBnNgDCwP6tzAO3vKEclxAtVxSaYX6zNpQJa0lgLoVq1UQAAQCAAmCjAA///AYf9D39D6vZDx+AKfD4oZNwWz3ApAwGCEFL71+1vAQAAAEDAAABQAAAgAAJ4AUAOMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMAAMCGx8GMTx6PuwLWBjdAam9tgAAAABJRU5ErkJggg=="
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Limitations: Instability and Sensitivity\n",
        "\n",
        "Decision Trees tend to be unstable, meaning small changes in training data can lead to very different trees and predictions.\n",
        "This is due to the hierarchical greedy splitting process.\n",
        "\n",
        "Approaches to mitigate instability include:\n",
        "- Using ensemble methods like Random Forests\n",
        "- Increasing regularization\n",
        "- Using more robust splitting criteria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Exercises Overview\n",
        "\n",
        "The chapter suggests hands-on exercises such as:\n",
        "- Training and fine-tuning Decision Trees on the moons dataset\n",
        "- Implementing grid search cross-validation for hyperparameter tuning\n",
        "- Building forests of Decision Trees to observe ensemble performance gains\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "This notebook is a comprehensive summary of Decision Tree theory and practice with runnable examples for better understanding and further experimentation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}