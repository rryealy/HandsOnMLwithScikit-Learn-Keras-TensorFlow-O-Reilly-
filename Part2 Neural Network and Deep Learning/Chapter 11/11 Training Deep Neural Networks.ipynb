{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Deep Neural Networks: Comprehensive Summary\n",
        "## Chapter 11 - Training Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Deep Neural Networks (DNNs)\n",
        "- **Definition**: DNNs are neural networks with multiple hidden layers that can learn complex representations of data.\n",
        "- **Applications**: Used in various fields such as image recognition, natural language processing, and more.\n",
        "- **Challenges**: Training DNNs can be difficult due to issues like vanishing/exploding gradients, slow convergence, and overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Common Problems in Training DNNs\n",
        "### 2.1 Vanishing and Exploding Gradients\n",
        "- **Vanishing Gradients**: Gradients become very small, making it hard for the network to learn.\n",
        "- **Exploding Gradients**: Gradients become excessively large, causing the model to diverge.\n",
        "- **Solution**: Use activation functions like ReLU, which help mitigate these issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Overfitting\n",
        "- **Definition**: When a model learns noise in the training data instead of the underlying pattern.\n",
        "- **Symptoms**: High accuracy on training data but poor performance on validation/test data.\n",
        "- **Solutions**: Use techniques like dropout, L2 regularization, and early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Techniques to Train DNNs\n",
        "### 3.1 Weight Initialization\n",
        "- **Importance**: Proper initialization can prevent vanishing/exploding gradients.\n",
        "- **Methods**:\n",
        "  - **Xavier Initialization**: Suitable for sigmoid/tanh activations.\n",
        "  - **He Initialization**: Recommended for ReLU activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Weight Initialization in Keras\n",
        "from tensorflow import keras\n",
        "\n",
        "# Build a simple model with He initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', input_shape=(input_dim,)),\n",
        "    keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Batch Normalization\n",
        "- **Definition**: A technique to normalize the inputs of each layer to improve training speed and stability.\n",
        "- **Benefits**: Reduces internal covariate shift, allows for higher learning rates, and acts as a regularizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Batch Normalization in Keras\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Optimizers\n",
        "- **Gradient Descent Variants**: Different optimizers can significantly affect training speed and convergence.\n",
        "- **Common Optimizers**:\n",
        "  - **SGD (Stochastic Gradient Descent)**: Basic optimizer.\n",
        "  - **Adam**: Adaptive learning rate optimizer that combines the benefits of AdaGrad and RMSProp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of using Adam optimizer in Keras\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Regularization Techniques\n",
        "### 4.1 Dropout\n",
        "- **Definition**: A technique where randomly selected neurons are ignored during training to prevent overfitting.\n",
        "- **Implementation**: Specify a dropout rate (e.g., 0.5 for 50% of neurons to be dropped)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Dropout in Keras\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 L2 Regularization\n",
        "- **Definition**: Adds a penalty equal to the square of the magnitude of coefficients to the loss function.\n",
        "- **Purpose**: Helps to prevent overfitting by discouraging overly complex models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of L2 Regularization in Keras\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=regularizers.l2(0.01)),\n",
        "    keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conclusion\n",
        "- Training deep neural networks involves careful consideration of architecture, initialization, optimization, and regularization techniques.\n",
        "- Understanding these concepts is crucial for building effective models that generalize well to unseen data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
